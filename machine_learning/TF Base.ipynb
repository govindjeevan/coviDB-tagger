{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chicken-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "australian-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_tags = pd.read_csv(\"articles_tags.csv\")\n",
    "article_text = pd.read_csv(\"../resources/articles_text.csv\")\n",
    "article_tags= article_tags.set_index(\"Resource URL\")\n",
    "article_text = article_text.set_index(\"Resource URL\")\n",
    "article_text = article_text[~article_text.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alike-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in article_text.index:\n",
    "    article_text.loc[i, \"tech_tag\"] =  article_tags.loc[i][\"Technical (Y/N)\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "inappropriate-briefs",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pandas/core/indexing.py:1783: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n",
      "/opt/conda/lib/python3.8/site-packages/pandas/core/frame.py:4162: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "data = (article_text[(article_text[\"tech_tag\"]==\"Y\") | (article_text[\"tech_tag\"]==\"N\")])\n",
    "data_src = data\n",
    "data_src = data_src[[\"Resource Title\", \"text\", \"tech_tag\"]]\n",
    "data_src.loc[:, \"text\"] =  data_src[\"Resource Title\"] + \" \" +data_src[\"text\"]\n",
    "data_src.loc[:, \"tech_tag\"] = data_src[\"tech_tag\"].map({\"N\": 0, \"Y\":1})\n",
    "data_src.drop(\"Resource Title\", axis=1, inplace=True)\n",
    "data_src = data_src.dropna().reset_index(drop=True)\n",
    "data=data_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "honest-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "executed-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tough-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data.text, data.tech_tag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "seeing-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "\n",
    "binary_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "valued-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collect-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text =  dataset.map(lambda text, labels: text)\n",
    "binary_vectorize_layer.adapt(train_text)\n",
    "int_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "optical-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return binary_vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "integral-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return int_vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "three-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "julian-trunk",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text in /opt/conda/lib/python3.8/site-packages (2.5.0)\n",
      "Requirement already satisfied: tensorflow<2.6,>=2.5.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-text) (2.5.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-text) (0.12.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (2.5.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.34.1)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.12.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.19.5)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (2.5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.12)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.7.4.2)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.35.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.12.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.1.2)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.30.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (50.3.1.post20201107)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2.23.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2020.12.5)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "civic-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "electronic-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "offensive-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, unused_label):\n",
    "    lower_case = tf_text.case_fold_utf8(text)\n",
    "    return tokenizer.tokenize(lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "animal-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "infinite-innocent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  b'Efficacy o'\n",
      "Label: 1\n",
      "Sentence:  b'Effectiven'\n",
      "Label: 1\n",
      "Sentence:  b'Updated Ma'\n",
      "Label: 1\n",
      "Sentence:  b'The New En'\n",
      "Label: 1\n",
      "Sentence:  b'SARS-CoV-2'\n",
      "Label: 1\n",
      "Sentence:  b'Global Inc'\n",
      "Label: 1\n",
      "Sentence:  b'Missing th'\n",
      "Label: 0\n",
      "Sentence:  b'Mass-Vacci'\n",
      "Label: 0\n",
      "Sentence:  b'Antibody R'\n",
      "Label: 1\n",
      "Sentence:  b'Coronaviru'\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "for text, label in all_labeled_data.take(10):\n",
    "    print(\"Sentence: \", text.numpy()[:10])\n",
    "    print(\"Label:\", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "wicked-average",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = all_labeled_data.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "united-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "smaller-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "contained-juice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  10000\n",
      "First five vocab entries: [b'the', b',', b'of', b'.', b'and']\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = configure_dataset(tokenized_ds)\n",
    "\n",
    "vocab_dict = collections.defaultdict(lambda: 0)\n",
    "for toks in tokenized_ds.as_numpy_iterator():\n",
    "  for tok in toks:\n",
    "    vocab_dict[tok] += 1\n",
    "\n",
    "vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab = [token for token, count in vocab]\n",
    "vocab = vocab[:VOCAB_SIZE]\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "print(\"First five vocab entries:\", vocab[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "great-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = vocab\n",
    "values = range(2, len(vocab) + 2)  # reserve 0 for padding, 1 for OOV\n",
    "\n",
    "init = tf.lookup.KeyValueTensorInitializer(\n",
    "    keys, values, key_dtype=tf.string, value_dtype=tf.int64)\n",
    "\n",
    "num_oov_buckets = 1\n",
    "vocab_table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "permanent-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, label):\n",
    "    standardized = tf_text.case_fold_utf8(text)\n",
    "    tokenized = tokenizer.tokenize(standardized)\n",
    "    vectorized = vocab_table.lookup(tokenized)\n",
    "    return vectorized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "rental-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoded_data = all_labeled_data.map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "olympic-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SIZE=200\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "animated-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(VALIDATION_SIZE).shuffle(BUFFER_SIZE)\n",
    "validation_data = all_encoded_data.take(VALIDATION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aware-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.padded_batch(BATCH_SIZE)\n",
    "validation_data = validation_data.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "registered-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fresh-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = configure_dataset(train_data)\n",
    "validation_data = configure_dataset(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "natural-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "modern-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn(vocab_size, num_labels):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_labels)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_model(vocab_size, num_labels):\n",
    "    model = tf.keras.Sequential([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-practice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "optical-graphics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 18s 835ms/step - loss: 0.6786 - accuracy: 0.6550 - val_loss: 0.6524 - val_accuracy: 0.8150\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 8s 629ms/step - loss: 0.4977 - accuracy: 0.8512 - val_loss: 0.5090 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 8s 631ms/step - loss: 0.1896 - accuracy: 0.9375 - val_loss: 0.5623 - val_accuracy: 0.7900\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 8s 639ms/step - loss: 0.0904 - accuracy: 0.9737 - val_loss: 0.5032 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 8s 643ms/step - loss: 0.0448 - accuracy: 0.9875 - val_loss: 0.5710 - val_accuracy: 0.7850\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 8s 633ms/step - loss: 0.0224 - accuracy: 0.9937 - val_loss: 0.7186 - val_accuracy: 0.7800\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 8s 636ms/step - loss: 0.0092 - accuracy: 0.9987 - val_loss: 0.6531 - val_accuracy: 0.7900\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 8s 623ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.8970 - val_accuracy: 0.7700\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 8s 634ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.1254 - val_accuracy: 0.7700\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 8s 637ms/step - loss: 8.9096e-04 - accuracy: 1.0000 - val_loss: 1.1784 - val_accuracy: 0.7600\n"
     ]
    }
   ],
   "source": [
    "model = create_rnn(vocab_size=vocab_size, num_labels=2)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "history = model.fit(train_data, validation_data=validation_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-grain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-compression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-arbitration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-terry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "removed-interaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 5s 387ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.5819 - val_accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 5s 397ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.5966 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 5s 390ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.6095 - val_accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 5s 409ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.6220 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 5s 397ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 5s 397ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6450 - val_accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 5s 419ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6551 - val_accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 5s 405ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6649 - val_accuracy: 0.7500\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 5s 400ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 5s 398ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6831 - val_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, validation_data=validation_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "earned-possibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 27ms/step - loss: 0.5100 - accuracy: 0.8000\n",
      "Loss:  0.5099855661392212\n",
      "Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_data)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: {:2.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-halifax",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
